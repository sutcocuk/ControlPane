[Bundle Replication Failure]
action.email = 1
action.email.inline = 1
action.email.message.alert = The alert condition for '$name$' was triggered.\
\
\
1 van de search head cluster werkt niet qua replicatie. Een captain transfer lost dit issue meestal op.  Via de monitor console kan je zien bij de captains of de replicatie werkt.
action.email.sendresults = 1
action.email.to = support@smtware.com
action.email.useNSSubject = 1
action.webhook.enable_allowlist = 0
alert.suppress = 0
alert.track = 0
counttype = number of events
cron_schedule = 43 */1 * * 1-7
dispatch.earliest_time = -60m@m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","tag","tag::eventtype","message"]
display.page.search.mode = verbose
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = search
request.ui_dispatch_view = search
search = index=_internal  source=*/metrics.log* group=bundle_replication name=bundle_metadata bundle_type=full_bundle\
| lookup assets.csv host\
| mvexpand search_group\
| search search_group = *searchheadclustergroup*\
| stats dc(host) as dhost by host search_group\
\
| append \
    [ search index=_internal  source=*/metrics.log* group=bundle_replication name=bundle_metadata bundle_type=full_bundle earliest=-130m latest=-60m\
     | lookup assets.csv host\
     | mvexpand search_group\
     | search search_group = *searchheadclustergroup*\
     | stats dc(host) as dhost by host search_group]\
| stats c(dhost) as chost by host\
| where chost < 2

[Cribl failed task execution]
action.email = 1
action.email.inline = 1
action.email.message.alert = The alert condition for '$name$' was triggered.\
\
Als gevolg van meerdere skipped (hangende) jobs of overige fouten wordt datainput via cribl niet meer binnengehaald. Ga na op basis van de reason wat de vervolgacties zijn om dit te verhelpen.\
\
IN GEVAL VAN 504 ERRORS KAN HET WIJZEN OP EEN TIME-OUT OP DE PROXY, EEN HERSTART VAN DE WORKER KAN UITKOMST BIEDEN.
action.email.sendresults = 1
action.email.to = support@smtware.com
action.email.useNSSubject = 1
action.webhook.enable_allowlist = 0
alert.suppress = 0
alert.track = 0
counttype = number of events
cron_schedule = 0 2 * * *
description = Als gevolg van meerdere skipped (hangende) jobs of overige fouten wordt datainput via cribl niet meer binnengehaald. Ga na op basis van de reason wat de vervolgacties zijn om dit te verhelpen.
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","tag","tag::eventtype","message"]
display.general.type = statistics
display.page.search.mode = verbose
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = ekesh
request.ui_dispatch_view = search
search = index=cribl_logs ( api TaskExecutor error "failed to execute task" ) OR ( scheduledJob skipped consecutiveSkips!=1) NOT adhoc \
| eval job = coalesce(jobId,scheduledJob) , reason = if(isnull(reason),max(totalSkips)." skipped runs",reason)| stats latest(_time) as _time latest(reason) as reason  by job message

[Datamodel Partitie volume treshold exceeded]
action.email = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = support@smtware.com
action.email.useNSSubject = 1
action.webhook.enable_allowlist = 0
alert.suppress = 0
alert.track = 0
counttype = number of events
cron_schedule = 0 8 * * *
description = Wanneer een partitie op 1 indexer boven de 90% is kan dit duiden op aankomend storage terkort voor onze datamodellen. Een rebalance of overige maatregelen zijn ook gepast.
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","tag","tag::eventtype","message"]
display.general.type = statistics
display.page.search.mode = verbose
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = ekesh
request.ui_dispatch_view = search
search = | rest "services/data/index-volumes"\
| lookup assets.csv host as splunk_server\
| search search_group = *indexer*\
| eval dmc_volume=if(isnotnull(title),title,'data.name')\
| where NOT ('dmc_volume' == "_splunk_summaries")\
| fields - _dmc_volume \
| eval volumeSizeGB=if((total_size > 1),round((total_size / 1024),2),null()), sizeUsagePerc=((total_size / max_size) * 100)\
| search name=datamodel\
| where sizeUsagePerc > 90

[Mem OR Cpu Usage > 85% (p90)]
action.email = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = support@smtware.com
action.email.useNSSubject = 1
action.webhook.enable_allowlist = 0
alert.suppress = 1
alert.suppress.period = 4h
alert.track = 0
counttype = number of events
cron_schedule = 1-59/14 * * * *
dispatch.earliest_time = -15m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","tag","tag::eventtype","message"]
display.general.type = statistics
display.page.search.mode = verbose
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = ekesh
request.ui_dispatch_view = search
search = index="_introspection" sourcetype=splunk_resource_usage component=Hostwide \
|  stats  p90(data.mem) as mem p90(data.mem_used) as memused p90(data.cpu_system_pct) as systemcpu p90(data.cpu_user_pct) as usercpu  by host _time\
|  eval mempct = round(memused/mem*100,2) \
|  eval cpupct = systemcpu + usercpu \
|  stats max(cpupct) as cpu max(mempct) as mem by  host\
| where cpu > 90 OR mempct > 90

[Replicatie_Failure]
action.email = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = support@smtware.com
action.email.useNSSubject = 1
action.webhook.enable_allowlist = 0
alert.suppress = 0
alert.track = 0
counttype = number of events
cron_schedule = 13 2 * * *
dispatch.earliest_time = -1d@d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","tag","tag::eventtype","message"]
display.general.type = statistics
display.page.search.mode = verbose
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = ekesh
request.ui_dispatch_view = search
search = index=_internal sourcetype=splunkd "because replication was unsuccessful. replicationStatus Failed failure info: failed_because"\
| stats count, max(_time) AS mostRecent by host, message \
| eval mostRecent=strftime(mostRecent,"%+")

[Cold_Storage_vol]
action.email = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = support@smtware.com
action.email.useNSSubject = 1
action.webhook.enable_allowlist = 0
alert.suppress = 0
alert.track = 0
counttype = number of events
cron_schedule = 23 2 * * 1
dispatch.earliest_time = -1w@w
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","tag","tag::eventtype","message"]
display.page.search.mode = verbose
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = ekesh
request.ui_dispatch_view = search
search = index=_internal sourcetype=splunkd TERM(exceeds) TERM(volumemanager) TERM(trim) "Size exceeds max, will have to trim " NOT Primary\
| fields + host, _raw

[Splunk core servers are down]
action.email = 1
action.email.to = support@smtware.com
action.email.useNSSubject = 1
action.webhook.enable_allowlist = 0
alert.suppress = 0
alert.track = 0
counttype = number of events
cron_schedule = 2,17,32,47 * * * *
dispatch.earliest_time = -15m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","tag","tag::eventtype","message"]
display.general.type = statistics
display.page.search.mode = verbose
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = ekesh
request.ui_dispatch_view = search
search = | inputlookup assets.csv\
| dedup host\
| table host\
| join type=left host [search index=_internal sourcetype=splunkd | stats c as counts by host| table host counts]\
| where isnull(counts)

[Indexer_Issue]
action.email = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = support@smtware.com
action.email.useNSSubject = 1
action.webhook.enable_allowlist = 0
alert.suppress = 1
alert.suppress.period = 30m
alert.track = 0
alert_condition = search count > 5
counttype = custom
cron_schedule = 3,18,33,48 * * * *
description = De Manager kan geen data kwijt bij een indexer
dispatch.earliest_time = -15m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","tag","tag::eventtype","message"]
display.general.type = statistics
display.page.search.mode = verbose
display.page.search.tab = statistics
enableSched = 1
request.ui_dispatch_app = ekesh
request.ui_dispatch_view = search
search = index=_internal sourcetype=splunkd eventType=connect_fail \
[| inputlookup assets.csv\
| search search_group="dmc_group_cluster_master"\
| table host]\
| stats count by destHost

[Indexer_Queue_vol]
action.email = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = support@smtware.com
action.email.useNSSubject = 1
action.webhook.enable_allowlist = 0
alert.suppress = 0
alert.track = 0
alert_condition = search severity_level>0
counttype = custom
cron_schedule = 57 4 * * *
description = Search for indexer errors
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","tag","tag::eventtype","message"]
display.general.type = statistics
display.page.search.mode = verbose
display.page.search.tab = statistics
enableSched = 1
request.ui_dispatch_app = ekesh
request.ui_dispatch_view = search
search = | rest /services/server/introspection/indexer\
 | fields splunk_server, status, reason\
     | eval severity_level = if(status == "normal", 0, 2)\
     | rename splunk_server as instance

[Internal_Errors]
action.email = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = support@smtware.com
action.email.useNSSubject = 1
action.webhook.enable_allowlist = 0
alert.digest_mode = 0
alert.suppress = 1
alert.suppress.fields = email_subj
alert.suppress.period = 1h
alert.track = 0
counttype = number of events
cron_schedule = 2-58/10 * * * *
description = Groepering van een aantal _internal error meldingen waarop actie ondernomen dient te worden.\
Dit is nu nog:\
- Low disk space\
- KVStore Process Terminated\
- THP enabled\
- Ulimit below 60000
dispatch.earliest_time = -15m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","tag","tag::eventtype","message"]
display.general.type = statistics
display.page.search.mode = verbose
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = ekesh
request.ui_dispatch_view = search
search = index=_internal  sourcetype=splunkd   NOT DispatchManager \
    "The minimum free disk space" OR "No space left on device" OR "KV Store process terminated" OR  ("Linux transparent hugepage support, enabled="  enabled!="never") OR ("ulimit - Limit: open files") \
[| inputlookup assets.csv | dedup host | table host\
| append [| makeresults |eval host = `HF_SUF_hosts`| makemv delim="," host| mvexpand host|table host]]\
| table _raw host

[alerting skipped searches]
action.email = 1
action.email.inline = 1
action.email.sendresults = 1
action.email.to = support@smtware.com
action.email.useNSSubject = 1
action.webhook.enable_allowlist = 0
alert.suppress = 0
alert.track = 0
counttype = number of events
cron_schedule = */6 * * * *
dispatch.earliest_time = -60m@m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","tag","tag::eventtype","message"]
display.page.search.mode = verbose
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = ekesh
request.ui_dispatch_view = search
search = index=_internal status=skipped source="/opt/splunk/var/log/splunk/scheduler.log" NOT "*concurrent*"

[Lookup assets.csv geshared]
action.email = 1
action.email.inline = true
action.email.message.alert = The alert condition for '$name$' was triggered.\
\
De lookup assets.csv is niet geshared naar Global. Dit moet naar Global.
action.email.sendpdf = false
action.email.sendresults = true
action.email.to = support@smtware.com
action.email.track_alert = true
action.email.useNSSubject = 1
action.email.use_ssl = false
action.email.use_tls = true
action.email.width_sort_columns = true
action.webhook.enable_allowlist = 0
alert.digest_mode = false
alert.suppress = false
alert.suppress.period = 60s
alert.track = false
counttype = number of events
cron_schedule = 0 9 * * *
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","tag","tag::eventtype","message"]
display.general.type = statistics
display.page.search.mode = verbose
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = search
request.ui_dispatch_view = search
search = |rest splunk_server=local /services/data/lookup-table-files/assets.csv\
| stats c\
| eval status = if(c = 0, "Not Good", "Good")\
| search status = "Not Good"

[Splunk HF's/SUF's are down]
action.email = 1
action.email.message.alert = The alert condition for '$name$' was triggered.
action.email.to = support@smtware.com
action.email.useNSSubject = 1
action.webhook.enable_allowlist = 0
alert.suppress = 0
alert.suppress.period = 60s
alert.track = 0
counttype = number of events
cron_schedule = 2,17,32,47 * * * *
dispatch.earliest_time = -15m
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","tag","tag::eventtype","message"]
display.general.type = statistics
display.page.search.mode = verbose
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = ekesh
request.ui_dispatch_view = search
search = | makeresults\
| eval hosts = `HF_SUF_hosts`\
| makemv delim="," hosts\
| mvexpand hosts\
| rename hosts as host\
| join max=0 host type=left [| search index=_internal sourcetype=splunkd | stats c as counts by host ]\
| table host counts\
| search NOT host="NVT"\
| where isnull(counts)\
| search NOT host="DEFINIEER_HF_en_Intermediate_HOSTS" OR host="NVT"

[Macro waardes toevoegen]
action.email = 1
action.email.inline = 1
action.email.message.alert = The alert condition for '$name$' was triggered.\
\
Juiste waardes toekennen aan de macro's.\
\
 \
title: HF_SUF_hosts de HF's en evt. intermediate hosts toevoegen, bij geen, dan het volgende toevoegen "NVT"\
\
title:klantnaam, de Klantnaam toevoegen.
action.email.sendresults = 1
action.email.to = support@smtware.com
action.email.useNSSubject = 1
action.webhook.enable_allowlist = 0
alert.suppress = 0
alert.track = 0
counttype = number of events
cron_schedule = 0 9 * * *
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","tag","tag::eventtype","message"]
display.general.type = statistics
display.page.search.mode = verbose
display.page.search.tab = statistics
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = ekesh
request.ui_dispatch_view = search
search = |rest splunk_server=local /services/data/macros \
| search eai:acl.app = "ekesh"\
| table title definition\
| search  definition = "*DEFINIEER*" OR definition= "KLANTNAAM"
